{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'19] assignment 2: learning with graphs — feedback\n",
    "[ntds'19]: https://github.com/mdeff/ntds_2019\n",
    "\n",
    "[Clément Vignac](https://people.epfl.ch/clement.vignac), [EPFL LTS4](https://lts4.epfl.ch),\n",
    "[Guillermo Ortiz Jiménez](https://gortizji.github.io), [EPFL LTS4](https://lts4.epfl.ch),\n",
    "[Volodymyr Miz](http://miz.space), [EPFL LTS2](https://lts2.epfl.ch), and\n",
    "[Nicolas Aspert](https://people.epfl.ch/nicolas.aspert), [EPFL LTS2](https://lts2.epfl.ch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Graph construction (7 points)\n",
    "\n",
    "Most of the answers were correct. Despite the fact that [detailed instructions and the answer to this question were on Moodle](https://moodle.epfl.ch/mod/forum/discuss.php?d=21876), there were a few mistakes. A common mistake was to choose a large `epsilon`. Some teams had a bad quality of graph (not sparse enough).\n",
    "\n",
    "## Q2: Laplacian (2.5 points)\n",
    "\n",
    "All teams succeeded in implementing a function to compute Laplacian matrices.\n",
    "\n",
    "## Q3: Eigendecomposition (1 point)\n",
    "\n",
    "Everyone has implemented this correctly. However, some teams chose `np.linalg.eigh` while others went for `np.linalg.eig` with explicit sorting after it. `np.linalg.eigh` is the most reasonable choice for this problem considering the fact that Laplacian is symmetric. On top of that, it returns a sorted result which makes your code look cleaner.  The main advantage of `np.linalg.eig` is that it works with non-symmetric matrices (while `np.linalg.eigh` does not) which makes your solution more universal. Although, in this case, it was unnecessary. All in all, we did not penalize the grade for the teams who chose `np.linalg.eig`.\n",
    "\n",
    "## Q4: Interpretation (2.5 points)\n",
    "\n",
    "Most teams answered this question correctly. A common mistake was to answer that combinatorial Laplacian provides better numerical stability.\n",
    "\n",
    "## Q5: Connected components (1 point)\n",
    "\n",
    "The majority has implemented this function correctly. One team did not test their code and removed the test cell that was provided in the assignment. Some teams have chosen a very high threshold.\n",
    "\n",
    "## Q6: Spectral clustering - baseline (3 points)\n",
    "\n",
    "### Code (1 point)\n",
    "\n",
    "All teams succeeded in implementing k-means clustering.\n",
    "\n",
    "### Why does k-means fails on the 'two moons' dataset? (2 points)\n",
    "\n",
    "Most teams answered correctly, i.e. that k-means expect clusters that are convex and isotropic (i.e. roughly ball-shaped) and performs poorly with the elongated shapes present in the dataset. Up to 1 point was removed when the explanations were correct but confuse or unclear. \n",
    "\n",
    "## Q7: Spectral clustering - implementation (5 points)\n",
    "\n",
    "### Code (3 points)\n",
    "\n",
    "The majority of implementations were correct. Spectral clustering (as opposed to laplacian eigenmaps) do not require to ignore the eigenvectors associated with the zero eigenvalue(s) (makes you lose half a point). When performing the normalized version, you not only have to use the normalized laplacian, but also you need to normalize the extracted *rows* of the eigenvectors. Some teams were performing normalization of the eigenvectors prior to their extraction, or sometimes not at all (1 point removed). \n",
    "\n",
    "## Q8: Spectral clustering - own dataset (4 points)\n",
    "\n",
    "All teams selected datasets that were performing badly with k-means, and worked with spectral clustering. Building the graph for the new dataset was sometimes sub-optimal (same issues as in the first part of the assignment). A few teams were quite creative with the datasets. We expected a few comments on the outcome of spectral clustering, i.e. something more than \"it does not work with k-means but works with spectral clustering\" (1 point for good comments) and it was asked to plot results of spectral clustering with both normalized and unnormalized version (0.5 point removed when one was missing).\n",
    "\n",
    "## Q9: Laplacian eigenmaps (4 points)\n",
    "\n",
    "### What does the algorithm preserve? (1 point)\n",
    "\n",
    "Almost all teams explained that Laplacian eigenmaps aim at preserving locality.\n",
    "However, it is important to also explain the motivation for this: Laplacian eigenmaps is based on the assumption that the data lies on a possibly non-linear manifold.\n",
    "By preserving only the local geometry of the data, it is possibly to capture the intrisic geometry of this manifold.\n",
    "\n",
    "### Code (1 point)\n",
    "\n",
    "In general, Laplacian eigenmaps should skip the eigenvectors corresponding to eigenvalue zero. The best way to do it is to use the function compute_number_connected_components implemented earlier. If there are several connected components, they may be more than one eigenvector to skip.\n",
    "However, since it was stated in the assignment that graphs should be connected, this error was penalized very lightly.\n",
    "\n",
    "### Plots (2 points)\n",
    "\n",
    "It is important to color each data point by its label. Otherwise, results are much harder to interpret.\n",
    "Laplacian eigenmaps don't work great on MNIST (compared to T-SNE).\n",
    "\n",
    "The most common mistake was to have plots looking almost like two or three straight lines. This is due to the graph construction. If the graph is not connected and that the code of Laplacian eigenmaps does not take it into account, each connected component will be constant along one axis. Plots where most points are *almost* constant along one axis may be due to a graph that is very weakly connected.\n",
    "\n",
    "## Q10: Other dimensionality reduction techniques (2 points)\n",
    "\n",
    "There were very few mistakes. The few ones usually came from a very approximate explanation of PCA. PCA is a linear method that finds an orthogonal projection of the data such that the variance of the projected point cloud is maximized. Almost all teams noticed that t-SNE performs much better on MNIST.\n",
    "\n",
    "## Q11: Frequencies (2 points)\n",
    "\n",
    "All teams have understood that smoothness is associated with low-eigenvalues. However, there were more mistakes in the way to measure smoothness:\n",
    "  * A few answers were based on the mean value of the signal. They are totally incorrect.\n",
    "  * More groups proposed to measure the variance of the signal on the graph. Whereas this is a better idea, it is not satisfactory. Different signals with the same variance can have different smoothness depending on the graph. For example, on a path graph of size 4, the signal 5 - 5 - 0 - 0 is more smooth than the signal 5 - 0 - 5 - 0, whereas they have the same variance.\n",
    "  * Another incorrect answer was to \"look at the eigenvalues of the signal\". A *graph* has eigenvalues that can be computed using the eigendecomposition of its Laplacian. However, a *signal on a graph* does not have eigenvalues. What is possible instead is to *represent the signal in the graph Fourier domain* by projecting the signal on each eigenvector. This leads us to the correct answer, which is:\n",
    "  * Compute the quadratic form of the Laplacian $x^T L x$. Note that this value corresponds to a \"non-smoothness\" (a value of smoothness could be its inverse). This operation has several interpretations:\n",
    "    * It can be seen as the computation of the square norm of the graph gradient, i.e. a weighted sum of the square norm of the signal difference at the extremities of each edge.\n",
    "    * It can also be seen as a quantity computed using the projection $p_\\lambda(x)$ of the signal on each eigenspace: $x^T L x = \\sum_{\\lambda \\in \\textit{eig}(G)} \\lambda \\| p_\\lambda(x) \\|^2 $\n",
    "    \n",
    "## Q12: Graph Fourier transform (2 points)\n",
    "\n",
    "All teams solved this question right.\n",
    "\n",
    "## Q13: Graph filters (3.5 points)\n",
    "\n",
    "### Ideal Tikhonov spectral response (1 point)\n",
    "\n",
    "All teams got this right.\n",
    "\n",
    "### Ideal graph filter (2 points)\n",
    "\n",
    "Most answers were right. However, some teams performed filtering directly on the vertex domain by multiplying the signal with the spectral response and that is totally incorrect.\n",
    "\n",
    "### Relationship between filtering and spectral decomposition (0.5 point)\n",
    "\n",
    "Most teams understood the interpretation of graph filtering as an operation that scales the coordinates of a graph signal in the basis given by the spectral decomposition of the Laplacian.\n",
    "In this sense, a low-pass filter only preserves the components associated with the smallest eigenvalues (and hence it smoothens the signal), a high-pass filter preserves the components associated with the largest eignevalues (and hence it produces signals with rapid spatial variations), and a band-pass filter preserves the components in between (and produces a mildly smooth signal).\n",
    "\n",
    "Looking at the spectral response of the Tikhonov filter we see that it weights down the components associated with large eigenvalues, and preserves the low frequencies. We thus say that this is a low-pass filter.\n",
    "\n",
    "Most erroneous answers came from teams who did not have this concept clear and that ended up writing too much, and unfortunately, showed some conceptual mistake. \n",
    "\n",
    "## Q14: Polynomial graph filters (7.5 points)\n",
    "\n",
    "### Fit polynomial (3 points)\n",
    "\n",
    "Most people had this right, and solved this question either by solving the least squares problem numerically using `np.linalg.lstsq`\n",
    "\n",
    "$$\\alpha = \\arg\\min_\\alpha||V\\alpha - h||_2^2$$\n",
    "\n",
    "or using the explicit solution in terms of the Moore-Penrose pseudo-inverse\n",
    "\n",
    "$$\\alpha = V^\\dagger h = (V^TV)^{-1}V^Th.$$\n",
    "\n",
    "However, some teams did not fully understand what they were doing and solved a combination of the two:\n",
    "\n",
    "$$\\alpha = \\arg\\min_\\alpha||V^TV\\alpha - V^Th||_2^2.$$\n",
    "\n",
    "Even if the final solution is the same, the extra multiplication of every term by $V^T$ is unnecessary and it shows a lack of understanding in what you are doing. We removed 1.5 points for this mistake.\n",
    "\n",
    "### Polynomial filter spectral response (2 points)\n",
    "\n",
    "Almost everybody had this right.\n",
    "\n",
    "### Polynomial filter (2 points)\n",
    "\n",
    "The main recurrent mistake was to perform filtering in the spectral domain instead of explicitly computing the filter as a matrix polynomial. In this regard, please note that the power of a matrix is NOT the power of its elements, i.e.\n",
    "$$A^k=A\\cdot A^{k-1}=A\\cdot A\\cdots A$$\n",
    "    \n",
    "### Order (0.5 point)\n",
    "\n",
    "We accept any answer between 3 and 10. For polynomial filters, the order allows you to set the right trade-off between computational complexity and accuracy. The lower the order the faster to compute the filter (basically it is the number of times you need to multiply the Laplacian with itself) and the higher the order the better the polynomial can fit the ideal response.\n",
    "\n",
    "## Q15: ARMA filter (2 points)\n",
    "\n",
    "### ARMA filter (1 point)\n",
    "\n",
    "Everybody had this right.\n",
    "\n",
    "### Implement filtering operation (1 point)\n",
    "\n",
    "Almost all groups had this right. The only right answer was `x_tk_polynomial = g_tk @ x_noisy`. If the filtering was performed in the spectral response we granted 0  points.\n",
    "\n",
    "## Q16 Logistic regression (2 points)\n",
    "\n",
    "All teams implemented correctly logistic regression and most of them tuned the parameters to get validation/test accuracies greater than 55%.\n",
    "0.5 point was removed if validation/test accuracies were greater 50% but below 55% and one point if they were below 50%.\n",
    "\n",
    "## Q17 Handcrafted filter for features (2.5 points)\n",
    "\n",
    "Most teams got good answers about the type of filter to design: the assumption is that the signal is relatively smooth over the graph, hence a lowpass (ideal or Tikhonov) was accepted.\n",
    "However order of polynomial approximation had to stay within reasonable values (3 to 10), some teams used unnecessarily high order (0.5 point removed).\n",
    "\n",
    "## Q18 GNN filter training (1 point)\n",
    "\n",
    "All teams ran the experiment and got the expected filter (changing the hyperparameters for the training was not removing any point).\n",
    "\n",
    "## Q19: Graph neural networks (3 points)\n",
    "\n",
    "The goal of this part was not to achieve the best performance possible. Instead, it was to show that a neural network can be seen as a filtering process followed by a logisitic regression, the difference with standard machine learning being that the filters are trained instead of hand-picked. \n",
    "\n",
    "It was important to understand that in the two implementations of this section (one with Pytorch and one with Scikit-learn), the model is the same: it is a Laplacian polynomial followed by a logistic regression. Even in the \"graph neural network\" (in Pytorch), there is no non-linearity nor anything \"deep\".\n",
    "\n",
    "Any solution with a \"correct performance\" (> 75% accuracy on the test set) was accepted. The groups which achieved a lower performance had either picked very bad hyperparameters for the logistic regression, or chosen a polynomial order $K$ too big.\n",
    "\n",
    "The last question was the most difficult. Two elements were expected in the answer:\n",
    "  * The main difference is that the Pytorch model is \"end-to-end\", that is, the logistic regression and the filters are trained simultaneously. In scikit-learn, it is a two-step process.\n",
    "  * Then, there are some differences in the way the model is optimized: the regularizer is not the same, and the optimization procedure neither. You were not expected to spot all the differences, one of them was enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
